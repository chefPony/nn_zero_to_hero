{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNlUq+iM3x8IUm31kpbxjUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefPony/nn_zero_to_hero/blob/master/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Transformer from scratch\n",
        "\n",
        "## Load Data"
      ],
      "metadata": {
        "id": "HtBmp2VVvbBA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G3EnxEKDu8hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3fb5b74-b7ca-4f77-ff49-54643a7c33de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-22 13:12:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-09-22 13:12:28 (18.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "hh-bFBlsvsuz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x69QjSSIv0PF",
        "outputId": "3a2b924f-7f7c-4efb-ff5f-438545ef85c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {c: i for i, c in enumerate(chars)}\n",
        "itos = {i: c for c, i in stoi.items()}\n",
        "\n",
        "encode = lambda x: [stoi[c] for c in x]\n",
        "decode = lambda x: [itos[i] for i in x]\n",
        "print(vocab_size)\n",
        "print(\"\".join(chars))\n",
        "print(itos)\n",
        "print(stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At9PyaHvv4Mg",
        "outputId": "ba09faaa-3c0d-4299-c500-e52a8c3bd841"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoded_data = torch.tensor([stoi[c] for c in text])\n",
        "print(encoded_data.shape)\n",
        "print(encoded_data[:1000])\n",
        "\n",
        "train_data = encoded_data[:int(len(text) * 0.9)]\n",
        "val_data = encoded_data[int(len(text) * 0.9):]\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoRNiePAxAAv",
        "outputId": "bf1547d0-32b5-439a-f223-c108b36a15cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n",
            "torch.Size([1003854])\n",
            "torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "def get_batch(data: torch.Tensor, batch_size: int, block_size: int):\n",
        "  ix = torch.randint(low=0, high=data.shape[0] - block_size, size=(batch_size,))\n",
        "  xb = torch.stack([data[i : i+block_size] for i in ix]).to(device)\n",
        "  yb = torch.stack([data[i+1 : i+block_size+1] for i in ix]).to(device)\n",
        "  return xb, yb\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, batch_size, block_size, num_batches=100):\n",
        "  loss_tr, loss_va = torch.zeros((num_batches, )), torch.zeros((num_batches, ))\n",
        "  model.eval()\n",
        "  for k in range(num_batches):\n",
        "    xb_tr, yb_tr = get_batch(train_data, batch_size, block_size)\n",
        "    xb_va, yb_va = get_batch(val_data, batch_size, block_size)\n",
        "    yhtr, loss_tr[k] = model(xb_tr, yb_tr)\n",
        "    yhva, loss_va[k] = model(xb_va, yb_va)\n",
        "  model.train()\n",
        "  return loss_tr.mean().item(), loss_va.mean().item()\n",
        "\n",
        "xb, yb = get_batch(encoded_data, 4, 8)\n",
        "print(xb.shape)\n",
        "print(yb.shape)\n",
        "print(\"batch\")\n",
        "print(xb)\n",
        "print(\"target\")\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjgCPXCBxivS",
        "outputId": "28bf8a88-5c33-4c69-98dd-639e25d9f647"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8])\n",
            "torch.Size([4, 8])\n",
            "batch\n",
            "tensor([[42,  1, 58, 46, 59, 57,  1, 21],\n",
            "        [54, 56, 47, 43, 57, 58, 11,  0],\n",
            "        [49, 47, 52, 45, 12,  1, 58, 46],\n",
            "        [58, 46, 53, 59, 58,  1, 56, 43]], device='cuda:0')\n",
            "target\n",
            "tensor([[ 1, 58, 46, 59, 57,  1, 21,  1],\n",
            "        [56, 47, 43, 57, 58, 11,  0, 37],\n",
            "        [47, 52, 45, 12,  1, 58, 46, 53],\n",
            "        [46, 53, 59, 58,  1, 56, 43, 42]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline: Bigram Model"
      ],
      "metadata": {
        "id": "e1AUb0Zd5BhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.logits = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    logits = self.logits(x) # (B, T, C)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "       B, T, C = logits.shape\n",
        "       loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    B, T = idx.shape\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, _ = self(idx[:, -T:])  #(B, n_vocab)\n",
        "      probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "      next_idx = torch.multinomial(probs, 1)\n",
        "      idx = torch.cat([idx, next_idx], dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "xb, yb = get_batch(train_data, batch_size=8, block_size=2)\n",
        "bigram = BigramModel(vocab_size).to(device)\n",
        "print(bigram(xb, yb))\n",
        "print(\"\".join(decode(bigram.generate(torch.tensor([[0 , 0]], device=device), 100).squeeze().tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2laIlU05BFP",
        "outputId": "d3268935-46eb-4b25-ddee-0d3fc00d77d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[-0.1827,  0.0524, -1.8020,  ..., -0.4538,  0.6346, -1.4856],\n",
            "         [-1.1441,  0.3383,  1.6992,  ...,  0.9254,  1.4805,  0.3449]],\n",
            "\n",
            "        [[-0.1827,  0.0524, -1.8020,  ..., -0.4538,  0.6346, -1.4856],\n",
            "         [-1.2800,  0.1359, -1.2744,  ...,  1.1272,  0.5445, -0.2186]],\n",
            "\n",
            "        [[-1.0800,  1.4510, -0.3488,  ...,  2.1158,  0.2643, -0.2391],\n",
            "         [ 0.4121, -1.9089, -0.0616,  ..., -0.6875,  0.2056, -0.7192]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.4899, -1.5937,  0.9481,  ...,  0.8930,  1.6673, -1.0136],\n",
            "         [ 2.5165, -0.0862,  0.1101,  ..., -0.6886, -0.2301,  0.0784]],\n",
            "\n",
            "        [[ 1.0688,  1.0354, -1.0889,  ..., -0.9309, -0.7496, -1.1346],\n",
            "         [-1.1441,  0.3383,  1.6992,  ...,  0.9254,  1.4805,  0.3449]],\n",
            "\n",
            "        [[ 0.4310, -0.2231,  0.2790,  ..., -0.3801, -0.2620, -0.5226],\n",
            "         [-1.0800,  1.4510, -0.3488,  ...,  2.1158,  0.2643, -0.2391]]],\n",
            "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor(4.6520, device='cuda:0', grad_fn=<NllLossBackward0>))\n",
            "\n",
            "\n",
            "n\n",
            "3H :X;hnzbzDP'VnTwg-a'EYoUddXR3DOZDCk,XY:xcDJ,u!HyIKimUgP-cAS3FKlRpvwi3CgMIKITjFYoAgVe3PfAEYVu.WGI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1.\n",
        "block_size = 2\n",
        "batch_size = 16\n",
        "n_iter = 20000\n",
        "model = BigramModel(vocab_size).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1.)\n",
        "\n",
        "for i in range(n_iter):\n",
        "  xb, yb = get_batch(train_data, batch_size, 2)\n",
        "  _, loss = model(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % 1000 == 0:\n",
        "    loss_tr, loss_va = evaluate_model(model, batch_size, block_size)\n",
        "    print(f\"{i}/{n_iter} Train: {loss_tr:.4f} Validation: {loss_va:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9vQRDs28OTL",
        "outputId": "be3b714f-930e-4d66-800b-d344aa7f7f66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/20000 Train: 4.3754 Validation: 4.4458\n",
            "1000/20000 Train: 3.7335 Validation: 3.9622\n",
            "2000/20000 Train: 3.7469 Validation: 3.9400\n",
            "3000/20000 Train: 3.6187 Validation: 3.6909\n",
            "4000/20000 Train: 3.8301 Validation: 3.8622\n",
            "5000/20000 Train: 3.8224 Validation: 3.9138\n",
            "6000/20000 Train: 3.6752 Validation: 3.9924\n",
            "7000/20000 Train: 3.8573 Validation: 3.9302\n",
            "8000/20000 Train: 3.7383 Validation: 3.7476\n",
            "9000/20000 Train: 3.7376 Validation: 3.9901\n",
            "10000/20000 Train: 3.9125 Validation: 4.0334\n",
            "11000/20000 Train: 3.9315 Validation: 3.9911\n",
            "12000/20000 Train: 3.6196 Validation: 3.7821\n",
            "13000/20000 Train: 3.7282 Validation: 3.9559\n",
            "14000/20000 Train: 3.9681 Validation: 4.1536\n",
            "15000/20000 Train: 3.7993 Validation: 4.0437\n",
            "16000/20000 Train: 3.7760 Validation: 3.8002\n",
            "17000/20000 Train: 3.8936 Validation: 3.8511\n",
            "18000/20000 Train: 3.5515 Validation: 3.6870\n",
            "19000/20000 Train: 3.6228 Validation: 3.8555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\".join(decode(model.generate(torch.zeros((1, 1), device=device).long(), 1000).squeeze().tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqNg7VUyHhWD",
        "outputId": "258b80cf-136d-4d78-974c-2d0d11754a1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Thin inorde\n",
            "WA falat mat E:\n",
            "Anothot thind me.\n",
            "A ngur, hat ghe me machavy-g houe.\n",
            "\n",
            "Fre nge be hore ckese we WA m'st RD:\n",
            "He avy qur, Bethas?\n",
            "\n",
            "WAR ming IN at laminorn GHe hoo R het R ROMartharng R ackest R R GHe bbele he qud pe, R t mue t he hy R R Rellllll hooring ham'stinot whar, w ste r, aknorinorvy st WAnode hivast me.\n",
            "te\n",
            "Thareas?\n",
            "WA R hing astr, atessotrinorulllam'shookel sellambe Rit at fatinornesorirorud amby t d I R hone fullld cke st hape d R in\n",
            "SThorstrt nostng nortartelamard R STue.\n",
            "He t n Rinorin I:\n",
            "acke g amastape\n",
            "Anoke WAR trit tre het\n",
            "Woonot RCl GHe ambule WAR faiche d E:\n",
            "Thorncke R RClle asorin R trorin he.\n",
            "\n",
            "Thal ID:\n",
            "Yorin tiee RWARist teloowe at le d WAD qud lstelotullllst R t fad, g st t atisor amering merorin nothe d nor WAR anorsod n ne harickn\n",
            "He forexchist What atot norstrinootckingr ne;\n",
            "WAR fusort pe am'st amam'st avy, I g soouetueld ortie;\n",
            "Whe Be amallatrngo g hackior nerinok R Mavy WARer Whothelld R it d t R Whasheleloo grustinosorie mir d havy:\n",
            "\n",
            "Thort R Wame t tu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer model"
      ],
      "metadata": {
        "id": "681Jvkn2HWgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class HeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, head_size, dropout=0):\n",
        "    super().__init__()\n",
        "    self.n_embd = n_embd\n",
        "    self.head_size = head_size\n",
        "    self.K = nn.Linear(self.n_embd, self.head_size, bias=False)\n",
        "    self.Q = nn.Linear(self.n_embd, self.head_size, bias=False)\n",
        "    self.V = nn.Linear(self.n_embd, self.head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size))))\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    B, T, C = x.shape\n",
        "    # x[i, j, :] stores the char and positional info for token[i, j]\n",
        "    # K, Q layers use that information to compute the query and key for each token\n",
        "    k, q = self.K(x), self.Q(x) # (B, T, H), (B, T, H)\n",
        "    v = self.V(x) # (B, T, v)\n",
        "    x = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, T)\n",
        "    # mask to disable flow of information from future tokens\n",
        "    x = x.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    x = F.softmax(x, dim=-1)\n",
        "    x = self.dropout(x)\n",
        "    x = x @ v # (B, T, v)\n",
        "    return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, n_heads, dropout=0):\n",
        "    super().__init__()\n",
        "    self.head_size = n_embd // n_heads\n",
        "    self.n_heads = n_heads\n",
        "    self.n_embd = n_embd\n",
        "    self.heads = nn.ModuleList([\n",
        "        HeadAttention(block_size, self.n_embd, self.head_size, dropout=dropout)\n",
        "        for _ in range(n_heads)])\n",
        "    self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    x = self.proj(x)\n",
        "    return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, eps=1e-8):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim, device=device)\n",
        "    self.beta = torch.zeros(dim, device=device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mu = torch.mean(x, dim=-1, keepdim=True) # (B, T, 1)\n",
        "    # The paper used the non corrected variance\n",
        "    sigma = torch.var(x, dim=-1, correction=0, keepdim=True) # (B, T, 1)\n",
        "    # (B, T, C) (B)\n",
        "    x = (x - mu) * (sigma + self.eps)**-0.5 * self.gamma + self.beta\n",
        "    return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, activation, dropout=0):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.activation = activation\n",
        "    self.net = nn.Sequential(*[\n",
        "        nn.Linear(self.input_dim, self.hidden_dim),\n",
        "        activation,\n",
        "        nn.Linear(self.hidden_dim, self.output_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, n_heads, dropout=0):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_heads\n",
        "    self.ma = MultiHeadAttention(block_size, n_embd, n_heads, dropout)\n",
        "    self.fa = FeedForward(n_embd, n_embd, n_embd, nn.GELU(), dropout)\n",
        "    self.ln1 = LayerNorm(n_embd)\n",
        "    self.ln2 = LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # skip connections\n",
        "    x = x + self.ma(self.ln1(x))\n",
        "    x = x + self.fa(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, n_blocks, n_heads, dropout=0):\n",
        "    super().__init__()\n",
        "    self.block_size = block_size\n",
        "    self.C = nn.Embedding(vocab_size, n_embd)\n",
        "    self.P = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[\n",
        "        Block(block_size, n_embd, n_heads, dropout)\n",
        "        for _ in range(n_blocks)])\n",
        "    self.ln = LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "    self.register_buffer(\"pos\", torch.arange(0, self.block_size, 1))\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    B, T = x.shape\n",
        "    x = self.C(x) + self.P(self.pos[:T])\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln(x)\n",
        "    logits = self.lm_head(x)\n",
        "    if y is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      loss = F.cross_entropy(logits.view(B*T, C), y.view(B*T))\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad\n",
        "  def generate(self, idx, max_new_tokens=100):\n",
        "    self.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, _ = self(idx[:, -self.block_size:])  #(B, n_vocab)\n",
        "      probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "      next_idx = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat([idx, next_idx], dim=1)\n",
        "    self.train()\n",
        "    return idx\n",
        "\n",
        "x = torch.randint(0, 65, size=(4, 8), device=device)\n",
        "l = Transformer(8, 16, 2, 2).to(device)\n",
        "l(x)[0].shape\n",
        "\n",
        "out = l.generate(torch.zeros((1, 1), device=device, dtype=torch.long), 100)\n",
        "for o in out:\n",
        "  print(\"\".join(decode(o.tolist())))\n",
        "  print(\"-------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQkZQk2zHZ5e",
        "outputId": "ce7e7d41-63d3-49b2-8450-36db9989f1e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FQiXg ;GYK;QhCXgZzmSj:jJnSEVi,Pd,AaxSsKB,mG&'h;:jccYr?nw,e!I,CY-3cZ UJ'l?FFdmJrakVlWzNMzESAf;PEECwVF\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 5000\n",
        "batch_size = 32\n",
        "block_size = 32\n",
        "embd = 64\n",
        "n_blocks = 4\n",
        "n_heads = 4\n",
        "dropout = 0.\n",
        "eval_interval = 200\n",
        "eval_batches = 200\n",
        "\n",
        "print(device)\n",
        "model = Transformer(block_size, embd, n_blocks, n_heads, dropout).to(device)\n",
        "print(f\"Parameters {sum(p.numel() for p in model.parameters())}\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "\n",
        "for i in range(n_iter):\n",
        "  xb, yb = get_batch(train_data, batch_size, block_size)\n",
        "  yhat, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % eval_interval == 0:\n",
        "    loss_tr, loss_va = evaluate_model(model, batch_size, block_size, eval_batches)\n",
        "    print(f\"Step {i}/{n_iter} Train: {loss_tr:.4f} Validation: {loss_va:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lTg1ANnIhnP",
        "outputId": "9da91df9-c343-407c-dcca-b84e62399ce7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Parameters 109505\n",
            "Step 0/5000 Train: 4.2083 Validation: 4.2181\n",
            "Step 200/5000 Train: 2.4857 Validation: 2.4891\n",
            "Step 400/5000 Train: 2.3223 Validation: 2.3298\n",
            "Step 600/5000 Train: 2.2150 Validation: 2.2328\n",
            "Step 800/5000 Train: 2.1209 Validation: 2.1566\n",
            "Step 1000/5000 Train: 2.0502 Validation: 2.0925\n",
            "Step 1200/5000 Train: 2.0008 Validation: 2.0581\n",
            "Step 1400/5000 Train: 1.9486 Validation: 2.0264\n",
            "Step 1600/5000 Train: 1.9158 Validation: 2.0032\n",
            "Step 1800/5000 Train: 1.8762 Validation: 1.9796\n",
            "Step 2000/5000 Train: 1.8500 Validation: 1.9584\n",
            "Step 2200/5000 Train: 1.8223 Validation: 1.9590\n",
            "Step 2400/5000 Train: 1.8030 Validation: 1.9336\n",
            "Step 2600/5000 Train: 1.7747 Validation: 1.9218\n",
            "Step 2800/5000 Train: 1.7675 Validation: 1.9028\n",
            "Step 3000/5000 Train: 1.7581 Validation: 1.8790\n",
            "Step 3200/5000 Train: 1.7283 Validation: 1.8719\n",
            "Step 3400/5000 Train: 1.7232 Validation: 1.8781\n",
            "Step 3600/5000 Train: 1.7156 Validation: 1.8539\n",
            "Step 3800/5000 Train: 1.7008 Validation: 1.8453\n",
            "Step 4000/5000 Train: 1.6846 Validation: 1.8429\n",
            "Step 4200/5000 Train: 1.6761 Validation: 1.8242\n",
            "Step 4400/5000 Train: 1.6676 Validation: 1.8377\n",
            "Step 4600/5000 Train: 1.6649 Validation: 1.8265\n",
            "Step 4800/5000 Train: 1.6487 Validation: 1.8107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = torch.zeros((1, 1), device=device).long()\n",
        "out = model.generate(v, 1000)\n",
        "print(\"\".join(decode(out[0, :].tolist())))\n",
        "print(\"-------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUD5ATowQi2m",
        "outputId": "6054d4e2-2a33-4802-c395-7c78f2dd8c40"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mird, o' be alie; pointise and stray unler the that hares of these rian, he hence's lake, caDe,--\n",
            "I'll be joy!\n",
            "\n",
            "POLINGBRET:\n",
            "Morest you she whom of this\n",
            "For may some thou haven shay the has was great!\n",
            "As are three thus jourmer; there thou go, besing toble.\n",
            "Let deliff my norts, he Towgs? weak one bound acconstandines years and not for itell come;\n",
            "Nid then matter was he dow would mother is his madand mosiness of him.\n",
            "\n",
            "Securs Murdencehed actit rest.\n",
            "Therefore Send well, liboodver her unno say\n",
            "Untitineven\n",
            "WoUt another persite on as this sun,\n",
            "Dett to thumb. Cawilly thou younding of thee\n",
            "one you but can the're's nove sake Strorse,\n",
            "Sicicilt detule meets to the changes reds steechar?\n",
            "'Thy is those no hole the are bafore ansford?\n",
            "He daid noble frear dies lo;\n",
            "And I am atthe stake it out all gentlent commindedness,\n",
            "You queeth'd mend het's lead: my aragely bother.\n",
            "\n",
            "YORK:\n",
            "BENVINt:\n",
            "I'rry, and Lords this desere slight.\n",
            "\n",
            "MERCUTIO:\n",
            "No, nustievos whithe shall up, and abins formoth!\n",
            "What lowers them oppea\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}