{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiZ2P/ZZuzalmV79nLSUH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefPony/nn_zero_to_hero/blob/master/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Transformer from scratch\n",
        "\n",
        "## Load Data"
      ],
      "metadata": {
        "id": "HtBmp2VVvbBA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G3EnxEKDu8hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df40d795-3710-4784-e182-ce2cae6230d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-15 10:39:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-15 10:39:36 (22.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "hh-bFBlsvsuz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x69QjSSIv0PF",
        "outputId": "41d22f37-8484-4ade-e0bd-4b1cf52d7264"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {c: i for i, c in enumerate(chars)}\n",
        "itos = {i: c for c, i in stoi.items()}\n",
        "\n",
        "encode = lambda x: [stoi[c] for c in x]\n",
        "decode = lambda x: [itos[i] for i in x]\n",
        "print(vocab_size)\n",
        "print(\"\".join(chars))\n",
        "print(itos)\n",
        "print(stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At9PyaHvv4Mg",
        "outputId": "f6005c1c-3717-4da5-9b62-818bf5604a0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoded_data = torch.tensor([stoi[c] for c in text])\n",
        "print(encoded_data.shape)\n",
        "print(encoded_data[:1000])\n",
        "\n",
        "train_data = encoded_data[:int(len(text) * 0.9)]\n",
        "val_data = encoded_data[int(len(text) * 0.9):]\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoRNiePAxAAv",
        "outputId": "fe1572d1-21e3-448e-a140-050fa76054ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n",
            "torch.Size([1003854])\n",
            "torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "def get_batch(data: torch.Tensor, batch_size: int, block_size: int):\n",
        "  ix = torch.randint(low=0, high=data.shape[0] - block_size, size=(batch_size,))\n",
        "  xb = torch.stack([data[i : i+block_size] for i in ix]).to(device)\n",
        "  yb = torch.stack([data[i+1 : i+block_size+1] for i in ix]).to(device)\n",
        "  return xb, yb\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, batch_size, block_size, num_batches=100):\n",
        "  loss_tr, loss_va = torch.zeros((num_batches, )), torch.zeros((num_batches, ))\n",
        "  model.eval()\n",
        "  for k in range(num_batches):\n",
        "    xb_tr, yb_tr = get_batch(train_data, batch_size, block_size)\n",
        "    xb_va, yb_va = get_batch(val_data, batch_size, block_size)\n",
        "    yhtr, loss_tr[k] = model(xb_tr, yb_tr)\n",
        "    yhva, loss_va[k] = model(xb_va, yb_va)\n",
        "  model.train()\n",
        "  return loss_tr.mean().item(), loss_va.mean().item()\n",
        "\n",
        "xb, yb = get_batch(encoded_data, 4, 8)\n",
        "print(xb.shape)\n",
        "print(yb.shape)\n",
        "print(\"batch\")\n",
        "print(xb)\n",
        "print(\"target\")\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjgCPXCBxivS",
        "outputId": "5c36909b-cb19-4d65-d424-6b44e885ed5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8])\n",
            "torch.Size([4, 8])\n",
            "batch\n",
            "tensor([[42,  1, 58, 46, 59, 57,  1, 21],\n",
            "        [54, 56, 47, 43, 57, 58, 11,  0],\n",
            "        [49, 47, 52, 45, 12,  1, 58, 46],\n",
            "        [58, 46, 53, 59, 58,  1, 56, 43]], device='cuda:0')\n",
            "target\n",
            "tensor([[ 1, 58, 46, 59, 57,  1, 21,  1],\n",
            "        [56, 47, 43, 57, 58, 11,  0, 37],\n",
            "        [47, 52, 45, 12,  1, 58, 46, 53],\n",
            "        [46, 53, 59, 58,  1, 56, 43, 42]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline: Bigram Model"
      ],
      "metadata": {
        "id": "e1AUb0Zd5BhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.logits = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    logits = self.logits(x) # (B, T, C)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "       B, T, C = logits.shape\n",
        "       loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    B, T = idx.shape\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, _ = self(idx[:, -T:])  #(B, n_vocab)\n",
        "      probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "      next_idx = torch.multinomial(probs, 1)\n",
        "      idx = torch.cat([idx, next_idx], dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "xb, yb = get_batch(train_data, batch_size=8, block_size=2)\n",
        "bigram = BigramModel(vocab_size).to(device)\n",
        "print(bigram(xb, yb))\n",
        "print(\"\".join(decode(bigram.generate(torch.tensor([[0 , 0]], device=device), 100).squeeze().tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2laIlU05BFP",
        "outputId": "1500413c-af2a-42b8-ab7c-47a811e2829b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[-0.1827,  0.0524, -1.8020,  ..., -0.4538,  0.6346, -1.4856],\n",
            "         [-1.1441,  0.3383,  1.6992,  ...,  0.9254,  1.4805,  0.3449]],\n",
            "\n",
            "        [[-0.1827,  0.0524, -1.8020,  ..., -0.4538,  0.6346, -1.4856],\n",
            "         [-1.2800,  0.1359, -1.2744,  ...,  1.1272,  0.5445, -0.2186]],\n",
            "\n",
            "        [[-1.0800,  1.4510, -0.3488,  ...,  2.1158,  0.2643, -0.2391],\n",
            "         [ 0.4121, -1.9089, -0.0616,  ..., -0.6875,  0.2056, -0.7192]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.4899, -1.5937,  0.9481,  ...,  0.8930,  1.6673, -1.0136],\n",
            "         [ 2.5165, -0.0862,  0.1101,  ..., -0.6886, -0.2301,  0.0784]],\n",
            "\n",
            "        [[ 1.0688,  1.0354, -1.0889,  ..., -0.9309, -0.7496, -1.1346],\n",
            "         [-1.1441,  0.3383,  1.6992,  ...,  0.9254,  1.4805,  0.3449]],\n",
            "\n",
            "        [[ 0.4310, -0.2231,  0.2790,  ..., -0.3801, -0.2620, -0.5226],\n",
            "         [-1.0800,  1.4510, -0.3488,  ...,  2.1158,  0.2643, -0.2391]]],\n",
            "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor(4.6520, device='cuda:0', grad_fn=<NllLossBackward0>))\n",
            "\n",
            "\n",
            "n\n",
            "3H :X;hnzbzDP'VnTwg-a'EYoUddXR3DOZDCk,XY:xcDJ,u!HyIKimUgP-cAS3FKlRpvwi3CgMIKITjFYoAgVe3PfAEYVu.WGI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1.\n",
        "block_size = 2\n",
        "batch_size = 16\n",
        "n_iter = 20000\n",
        "model = BigramModel(vocab_size).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1.)\n",
        "\n",
        "for i in range(n_iter):\n",
        "  xb, yb = get_batch(train_data, batch_size, 2)\n",
        "  _, loss = model(xb, yb)\n",
        "\n",
        "  for p in model.parameters():\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  for p in model.parameters():\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  if i % 1000 == 0:\n",
        "    loss_tr, loss_va = evaluate_model(model, batch_size, block_size)\n",
        "    print(f\"Train: {loss_tr:.4f} Validation: {loss_va:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9vQRDs28OTL",
        "outputId": "93dd8a88-3978-459c-8f2b-7c99450cf09e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4.5873 Validation: 4.5982\n",
            "Train: 2.9631 Validation: 2.9713\n",
            "Train: 2.7652 Validation: 2.8126\n",
            "Train: 2.6840 Validation: 2.6590\n",
            "Train: 2.6346 Validation: 2.6148\n",
            "Train: 2.6407 Validation: 2.6385\n",
            "Train: 2.5762 Validation: 2.6064\n",
            "Train: 2.6007 Validation: 2.5371\n",
            "Train: 2.5443 Validation: 2.5141\n",
            "Train: 2.4977 Validation: 2.5641\n",
            "Train: 2.4992 Validation: 2.5806\n",
            "Train: 2.5534 Validation: 2.5942\n",
            "Train: 2.5080 Validation: 2.5164\n",
            "Train: 2.5214 Validation: 2.5781\n",
            "Train: 2.5237 Validation: 2.5296\n",
            "Train: 2.4991 Validation: 2.5481\n",
            "Train: 2.5105 Validation: 2.4803\n",
            "Train: 2.5386 Validation: 2.4941\n",
            "Train: 2.4832 Validation: 2.5315\n",
            "Train: 2.4947 Validation: 2.5348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\".join(decode(model.generate(torch.zeros((1, 1), device=device).long(), 1000).squeeze().tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqNg7VUyHhWD",
        "outputId": "58284449-378c-461b-fa7f-2c4e98e92c54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tod n,\n",
            "KI Miced tlakemangENofowhas\n",
            "\n",
            "Thind mean tr url hathant me machan fHe.\n",
            "Bju d iere--sthurore cesse wn ch me, wouss teee?\n",
            "Tmatoretha'll me herd timere larBRole!-\n",
            "Whind h heve she, ENVI an ndowessamy o be's lore we, ba ph, Rpeleme t,\n",
            "TETIDUS:\n",
            "Bearunghimabr g y Hockis thtin;\n",
            "Bee s mire akns t\n",
            "Thanes nd:\n",
            "\n",
            "\n",
            "JUSh oureme:\n",
            "KjLADUMIase VQXon my ferily atessotrilorenorard hoWeal'seand be, gZduth ntide.\n",
            "ASTI tored ve y ube I thellurs:\n",
            "Dad\n",
            "\n",
            "DUCoyoundy d tr wn, thirdsto;UPXndind card.\n",
            "Pr have S yorves,\n",
            "H. tinather pre be bod have kes,\n",
            "WR$wad br t, ardset\n",
            "Thin plur, he,\n",
            "\n",
            "\n",
            "\n",
            "IORil mecowirund ENTmayoncerkilon fe myo me hotrorirpllifive l I:\n",
            "We!\n",
            "I heree yos\n",
            "\n",
            "CI:\n",
            "S:\n",
            "D 'tanf mid\n",
            "Weit I tesueoomollomspe p ffrd,\n",
            "Tyscer oucond andrinu, eromerint. wh byoin I concesed n amerd ickne.\n",
            "Quppemar s, h tonghot nd, w, fieacou tr ter, plle porthar ch by aCKIN't;! anfube s oumMut qXo me or he-B'qurallotry, s: owind p ord hok isone y?\n",
            "IAr en, t:\n",
            "P\n",
            "MPOit\n",
            "AstertubY\n",
            "INCpuro gr pand VYele m h Heecourns pumade,\n",
            "amves t,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer model"
      ],
      "metadata": {
        "id": "681Jvkn2HWgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class HeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, head_size, dropout=0):\n",
        "    super().__init__()\n",
        "    self.n_embd = n_embd\n",
        "    self.head_size = head_size\n",
        "    self.K = nn.Linear(self.n_embd, self.head_size, bias=False)\n",
        "    self.Q = nn.Linear(self.n_embd, self.head_size, bias=False)\n",
        "    self.V = nn.Linear(self.n_embd, self.head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size))))\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    B, T, C = x.shape\n",
        "    # x[i, j, :] stores the char and positional info for token[i, j]\n",
        "    # K, Q layers use that information to compute the query and key for each token\n",
        "    k, q = self.K(x), self.Q(x) # (B, T, H), (B, T, H)\n",
        "    v = self.V(x) # (B, T, v)\n",
        "    x = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, T)\n",
        "    # mask to disable flow of information from future tokens\n",
        "    x = x.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    x = F.softmax(x, dim=-1)\n",
        "    x = self.dropout(x)\n",
        "    x = x @ v # (B, T, T) @ (B, T, v) => (B, T, v)\n",
        "    return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, n_heads, dropout=0):\n",
        "    super().__init__()\n",
        "    self.head_size = n_embd // n_heads\n",
        "    self.n_heads = n_heads\n",
        "    self.n_embd = n_embd\n",
        "    self.heads = nn.ModuleList([\n",
        "        HeadAttention(block_size, self.n_embd, self.head_size, dropout=dropout)\n",
        "        for _ in range(n_heads)])\n",
        "    self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    x = self.proj(x)\n",
        "    return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, eps=1e-8):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim, device=device)\n",
        "    self.beta = torch.zeros(dim, device=device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mu = torch.mean(x, dim=-1, keepdim=True) # (B, T, 1)\n",
        "    # The paper used the non corrected variance\n",
        "    sigma = torch.var(x, dim=-1, correction=0, keepdim=True) # (B, T, 1)\n",
        "    # (B, T, C) (B)\n",
        "    x = (x - mu) * (sigma + self.eps)**-0.5 * self.gamma + self.beta\n",
        "    return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, activation, dropout=0):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.activation = activation\n",
        "    self.net = nn.Sequential(*[\n",
        "        nn.Linear(self.input_dim, self.hidden_dim),\n",
        "        activation,\n",
        "        nn.Linear(self.hidden_dim, self.output_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, n_heads, dropout=0):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_heads\n",
        "    self.ma = MultiHeadAttention(block_size, n_embd, n_heads, dropout)\n",
        "    self.fa = FeedForward(n_embd, n_embd, n_embd, nn.GELU(), dropout)\n",
        "    self.ln1 = LayerNorm(n_embd)\n",
        "    self.ln2 = LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # skip connections\n",
        "    x = x + self.ma(self.ln1(x))\n",
        "    x = x + self.fa(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, n_embd, n_blocks, n_heads, dropout=0):\n",
        "    super().__init__()\n",
        "    self.block_size = block_size\n",
        "    self.C = nn.Embedding(vocab_size, n_embd)\n",
        "    self.P = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[\n",
        "        Block(block_size, n_embd, n_heads, dropout)\n",
        "        for _ in range(n_blocks)])\n",
        "    self.ln = LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "    self.register_buffer(\"pos\", torch.arange(0, self.block_size, 1))\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    B, T = x.shape\n",
        "    x = self.C(x) + self.P(self.pos[:T])\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln(x)\n",
        "    logits = self.lm_head(x)\n",
        "    if y is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      loss = F.cross_entropy(logits.view(B*T, C), y.view(B*T))\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad\n",
        "  def generate(self, idx, max_new_tokens=100):\n",
        "    self.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, _ = self(idx[:, -self.block_size:])  #(B, n_vocab)\n",
        "      probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "      next_idx = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat([idx, next_idx], dim=1)\n",
        "    self.train()\n",
        "    return idx\n",
        "\n",
        "x = torch.randint(0, 65, size=(4, 8), device=device)\n",
        "l = Transformer(8, 16, 2, 2).to(device)\n",
        "l(x)[0].shape\n",
        "\n",
        "out = l.generate(torch.zeros((1, 1), device=device, dtype=torch.long), 100)\n",
        "for o in out:\n",
        "  print(\"\".join(decode(o.tolist())))\n",
        "  print(\"-------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQkZQk2zHZ5e",
        "outputId": "ce7e7d41-63d3-49b2-8450-36db9989f1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FQiXg ;GYK;QhCXgZzmSj:jJnSEVi,Pd,AaxSsKB,mG&'h;:jccYr?nw,e!I,CY-3cZ UJ'l?FFdmJrakVlWzNMzESAf;PEECwVF\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 5000\n",
        "batch_size = 32\n",
        "block_size = 32\n",
        "embd = 64\n",
        "n_blocks = 4\n",
        "n_heads = 4\n",
        "dropout = 0.\n",
        "eval_interval = 200\n",
        "eval_batches = 200\n",
        "\n",
        "print(device)\n",
        "model = Transformer(block_size, embd, n_blocks, n_heads, dropout).to(device)\n",
        "print(f\"Parameters {sum(p.numel() for p in model.parameters())}\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "\n",
        "for i in range(n_iter):\n",
        "  xb, yb = get_batch(train_data, batch_size, block_size)\n",
        "  yhat, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % eval_interval == 0:\n",
        "    loss_tr, loss_va = evaluate_model(model, batch_size, block_size, eval_batches)\n",
        "    print(f\"Step {i}/{n_iter} Train: {loss_tr:.4f} Validation: {loss_va:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lTg1ANnIhnP",
        "outputId": "9da91df9-c343-407c-dcca-b84e62399ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Parameters 109505\n",
            "Step 0/5000 Train: 4.2083 Validation: 4.2181\n",
            "Step 200/5000 Train: 2.4857 Validation: 2.4891\n",
            "Step 400/5000 Train: 2.3223 Validation: 2.3298\n",
            "Step 600/5000 Train: 2.2150 Validation: 2.2328\n",
            "Step 800/5000 Train: 2.1209 Validation: 2.1566\n",
            "Step 1000/5000 Train: 2.0502 Validation: 2.0925\n",
            "Step 1200/5000 Train: 2.0008 Validation: 2.0581\n",
            "Step 1400/5000 Train: 1.9486 Validation: 2.0264\n",
            "Step 1600/5000 Train: 1.9158 Validation: 2.0032\n",
            "Step 1800/5000 Train: 1.8762 Validation: 1.9796\n",
            "Step 2000/5000 Train: 1.8500 Validation: 1.9584\n",
            "Step 2200/5000 Train: 1.8223 Validation: 1.9590\n",
            "Step 2400/5000 Train: 1.8030 Validation: 1.9336\n",
            "Step 2600/5000 Train: 1.7747 Validation: 1.9218\n",
            "Step 2800/5000 Train: 1.7675 Validation: 1.9028\n",
            "Step 3000/5000 Train: 1.7581 Validation: 1.8790\n",
            "Step 3200/5000 Train: 1.7283 Validation: 1.8719\n",
            "Step 3400/5000 Train: 1.7232 Validation: 1.8781\n",
            "Step 3600/5000 Train: 1.7156 Validation: 1.8539\n",
            "Step 3800/5000 Train: 1.7008 Validation: 1.8453\n",
            "Step 4000/5000 Train: 1.6846 Validation: 1.8429\n",
            "Step 4200/5000 Train: 1.6761 Validation: 1.8242\n",
            "Step 4400/5000 Train: 1.6676 Validation: 1.8377\n",
            "Step 4600/5000 Train: 1.6649 Validation: 1.8265\n",
            "Step 4800/5000 Train: 1.6487 Validation: 1.8107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = torch.zeros((1, 1), device=device).long()\n",
        "out = model.generate(v, 1000)\n",
        "print(\"\".join(decode(out[0, :].tolist())))\n",
        "print(\"-------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUD5ATowQi2m",
        "outputId": "6054d4e2-2a33-4802-c395-7c78f2dd8c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mird, o' be alie; pointise and stray unler the that hares of these rian, he hence's lake, caDe,--\n",
            "I'll be joy!\n",
            "\n",
            "POLINGBRET:\n",
            "Morest you she whom of this\n",
            "For may some thou haven shay the has was great!\n",
            "As are three thus jourmer; there thou go, besing toble.\n",
            "Let deliff my norts, he Towgs? weak one bound acconstandines years and not for itell come;\n",
            "Nid then matter was he dow would mother is his madand mosiness of him.\n",
            "\n",
            "Securs Murdencehed actit rest.\n",
            "Therefore Send well, liboodver her unno say\n",
            "Untitineven\n",
            "WoUt another persite on as this sun,\n",
            "Dett to thumb. Cawilly thou younding of thee\n",
            "one you but can the're's nove sake Strorse,\n",
            "Sicicilt detule meets to the changes reds steechar?\n",
            "'Thy is those no hole the are bafore ansford?\n",
            "He daid noble frear dies lo;\n",
            "And I am atthe stake it out all gentlent commindedness,\n",
            "You queeth'd mend het's lead: my aragely bother.\n",
            "\n",
            "YORK:\n",
            "BENVINt:\n",
            "I'rry, and Lords this desere slight.\n",
            "\n",
            "MERCUTIO:\n",
            "No, nustievos whithe shall up, and abins formoth!\n",
            "What lowers them oppea\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}